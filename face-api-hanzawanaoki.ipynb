{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Services Face サービス\n",
    "\n",
    "- [Face のドキュメント](https://docs.microsoft.com/ja-jp/azure/cognitive-services/face/)\n",
    "\n",
    "- [Azure Face サービスとは](https://docs.microsoft.com/ja-jp/azure/cognitive-services/face/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import io\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw\n",
    "# Azure SDK for Pythonに含まれるFaceAPIを取り扱うクラス\n",
    "# FaceClientはdetectやverificationなどの各種Face API機能を扱うためのクラス\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "# Cognitive Serviceの認証をAPIキーを通じて行うためのクラス\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from azure.cognitiveservices.vision.face.models import TrainingStatusType, Person, SnapshotObjectType, OperationStatusType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [FaceClient class\n",
    "](https://docs.microsoft.com/ja-jp/python/api/azure-cognitiveservices-vision-face/azure.cognitiveservices.vision.face.faceclient?view=azure-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = os.environ['FACE_SUBSCRIPTION_KEY']\n",
    "ENDPOINT = os.environ['FACE_ENDPOINT']\n",
    "\n",
    "# FaceAPIクライアントのインスタンス作成\n",
    "face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hanzawa1': 'images/hanzawa1.png',\n",
       " 'kurosaki1': 'images/kurosaki1.png',\n",
       " 'owada1': 'images/owada1.png',\n",
       " 'hanzawa2': 'images/hanzawa2.png',\n",
       " 'kurosaki2': 'images/kurosaki2.png',\n",
       " 'owada3': 'images/owada3.png',\n",
       " 'owada2': 'images/owada2.png',\n",
       " 'kurosaki3': 'images/kurosaki3.png',\n",
       " 'hanzawa3': 'images/hanzawa3.png'}"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facekeys = []\n",
    "facepaths = []\n",
    "\n",
    "for dirname, _, filenames in os.walk('images'):\n",
    "    for filename in filenames:\n",
    "        facekeys.append(filename.strip('.png'))\n",
    "        facepaths.append(os.path.join(dirname, filename))\n",
    "        # print(filename.strip('.png')+' = '+'\\''+os.path.join(dirname, filename)+'\\'')\n",
    "        \n",
    "face_path_dict = dict(zip(facekeys, facepaths))\n",
    "face_path_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [detect_with_stream](https://docs.microsoft.com/ja-jp/python/api/azure-cognitiveservices-vision-face/azure.cognitiveservices.vision.face.operations.faceoperations?view=azure-python#detect-with-stream-image--return-face-id-true--return-face-landmarks-false--return-face-attributes-none--recognition-model--recognition-01---return-recognition-model-false--detection-model--detection-01---custom-headers-none--raw-false--callback-none----operation-config-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込んだ画像を送信して顔検出\n",
    "detected_faces = []\n",
    "keys = []\n",
    "\n",
    "for key, face in face_path_dict.items():\n",
    "    with open(face, 'rb') as image: # バイナリモードrbで読み込み\n",
    "        keys.append(key)\n",
    "        detected_faces.append(\n",
    "            face_client.face.detect_with_stream(\n",
    "                image,\n",
    "                return_face_attributes=['age', 'gender', 'smile', 'glasses','emotion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hanzawa1': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ffb21c01520>],\n",
       " 'kurosaki1': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ffb9c743bb0>],\n",
       " 'owada1': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ffb9aacf580>],\n",
       " 'hanzawa2': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ffb9ade19a0>],\n",
       " 'kurosaki2': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ff9141f6a90>],\n",
       " 'owada3': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ffb9ade1760>],\n",
       " 'owada2': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ff9144e2190>],\n",
       " 'kurosaki3': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ff9c3d2da00>],\n",
       " 'hanzawa3': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ff9c3dc8ee0>]}"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_faces_dict = dict(zip(keys, detected_faces))\n",
    "detected_faces_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace object at 0x7ffb21c01520>]\n"
     ]
    }
   ],
   "source": [
    "# hanzawa1を指定\n",
    "print(detected_faces_dict['hanzawa1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'face_id': '14a77c62-d264-4c87-bd2a-1fe61e3ed57b', 'recognition_model': None, 'face_rectangle': <azure.cognitiveservices.vision.face.models._models_py3.FaceRectangle object at 0x7ffb9c425df0>, 'face_landmarks': None, 'face_attributes': <azure.cognitiveservices.vision.face.models._models_py3.FaceAttributes object at 0x7ff91439bbb0>}\n"
     ]
    }
   ],
   "source": [
    "print(detected_faces_dict['hanzawa1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'age': 37.0, 'gender': <Gender.male: 'male'>, 'smile': 0.002, 'facial_hair': None, 'glasses': <GlassesType.no_glasses: 'noGlasses'>, 'head_pose': None, 'emotion': <azure.cognitiveservices.vision.face.models._models_py3.Emotion object at 0x7ff91409a880>, 'hair': None, 'makeup': None, 'occlusion': None, 'accessories': None, 'blur': None, 'exposure': None, 'noise': None}\n"
     ]
    }
   ],
   "source": [
    "# hanzawa1の'age', 'gender', 'smile', 'glasses','emotion'\n",
    "print(detected_faces_dict['hanzawa1'][0].face_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'age': 48.0, 'gender': <Gender.male: 'male'>, 'smile': 0.982, 'facial_hair': None, 'glasses': <GlassesType.no_glasses: 'noGlasses'>, 'head_pose': None, 'emotion': <azure.cognitiveservices.vision.face.models._models_py3.Emotion object at 0x7ff9144e2640>, 'hair': None, 'makeup': None, 'occlusion': None, 'accessories': None, 'blur': None, 'exposure': None, 'noise': None}\n"
     ]
    }
   ],
   "source": [
    "# owada1の'age', 'gender', 'smile', 'glasses','emotion'\n",
    "print(detected_faces_dict['owada1'][0].face_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二つの画像が同じ人物かそうでないかを判定する「Face Verify」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hanzawa1': '14a77c62-d264-4c87-bd2a-1fe61e3ed57b',\n",
       " 'kurosaki1': '0d45d73f-b29f-48b0-84ff-8e943feb6aba',\n",
       " 'owada1': '33626298-e410-4bf6-8b5e-e75887b10c93',\n",
       " 'hanzawa2': 'aa051851-5ee3-4257-8e2a-6bbb9730fb2c',\n",
       " 'kurosaki2': 'ce2cf59a-d6bc-42d5-bac1-ef4ffc579558',\n",
       " 'owada3': '96be1e47-e5d2-4564-b102-15adf6c9702a',\n",
       " 'owada2': 'c7f1ed7f-2242-4480-af98-b26fb9d27aba',\n",
       " 'kurosaki3': '9ed35e62-4593-4a58-84bb-7a5f6ed4fc91',\n",
       " 'hanzawa3': '1d3ea00a-481f-43e6-ba13-0396468115be'}"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顔検出結果のFaceIDを取得する\n",
    "n = len(detected_faces)\n",
    "ids = []\n",
    "\n",
    "for i in range(0, n):\n",
    "    ids.append(detected_faces[i][0].face_id)\n",
    "\n",
    "face_id_dict = dict(zip(dict_all.keys(), ids))\n",
    "face_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14a77c62-d264-4c87-bd2a-1fe61e3ed57b'"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_id_dict['hanzawa1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'is_identical': False, 'confidence': 0.16937}\n"
     ]
    }
   ],
   "source": [
    "# 同じ人物かを判定する（半沢と大和田）\n",
    "verified = face_client.face.verify_face_to_face(face_id_dict['hanzawa1'], face_id_dict['owada1'])\n",
    "print(verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'is_identical': False, 'confidence': 0.28355}\n"
     ]
    }
   ],
   "source": [
    "# 同じ人物かを判定する（半沢と黒崎）\n",
    "verified = face_client.face.verify_face_to_face(face_id_dict['hanzawa1'], face_id_dict['kurosaki1'])\n",
    "print(verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'is_identical': True, 'confidence': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 同じ人物かを判定する（同じ画像を入れてみる）\n",
    "verified = face_client.face.verify_face_to_face(face_id_dict['hanzawa1'], face_id_dict['hanzawa1'])\n",
    "print(verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'is_identical': True, 'confidence': 0.6732}\n"
     ]
    }
   ],
   "source": [
    "# 同じ人物かを判定する（表情の似た同じ人物の画像を入れてみる）\n",
    "verified = face_client.face.verify_face_to_face(face_id_dict['hanzawa1'], face_id_dict['hanzawa2'])\n",
    "print(verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'is_identical': False, 'confidence': 0.43218}\n"
     ]
    }
   ],
   "source": [
    "# 同じ人物かを判定する（表情の違う同じ人物の画像を入れてみる）\n",
    "verified = face_client.face.verify_face_to_face(face_id_dict['hanzawa1'], face_id_dict['hanzawa3'])\n",
    "print(verified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習用Person Groupの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Create a new person group with specified personGroupId](https://docs.microsoft.com/ja-jp/python/api/azure-cognitiveservices-vision-face/azure.cognitiveservices.vision.face.operations.persongroupoperations?view=azure-python#create-person-group-id--name-none--user-data-none--recognition-model--recognition-01---custom-headers-none--raw-false----operation-config-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person Group IDを指定\n",
    "person_group_id = 'hanzawa_group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person_group初期化（念の為）\n",
    "face_client.person_group.delete(person_group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person Groupを作成する\n",
    "face_client.person_group.create(\n",
    "    person_group_id=person_group_id, \n",
    "    name='Person Group for LINE Bot'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'name': 'Person Group for LINE Bot', 'user_data': None, 'recognition_model': None, 'person_group_id': 'hanzawa_group'}\n"
     ]
    }
   ],
   "source": [
    "# Person Groupを確認\n",
    "hanzawa_group = face_client.person_group.get(person_group_id)\n",
    "print(hanzawa_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習用Personの作成\n",
    "\n",
    "#### 顔を入れるための箱を作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [create(person_group_id, name=None, user_data=None, custom_headers=None, raw=False, **operation_config)](https://docs.microsoft.com/ja-jp/python/api/azure-cognitiveservices-vision-face/azure.cognitiveservices.vision.face.operations.persongrouppersonoperations?view=azure-python#create-person-group-id--name-none--user-data-none--custom-headers-none--raw-false----operation-config-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名前を登録\n",
    "hanzawa_name = 'hanzawa_naoki'\n",
    "owada_name = 'owada_akira'\n",
    "kurosaki_name = 'kurosaki_syunichi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hanzawa_group'"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hanzawa_group.person_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': 'c13c5ca2-21ba-4ce1-bb73-7f1ec9460cf2', 'persisted_face_ids': None}\n"
     ]
    }
   ],
   "source": [
    "# Personを登録\n",
    "hanzawa = face_client.person_group_person.create(\n",
    "    person_group_id = hanzawa_group.person_group_id, # PersonGroupのIDを指定\n",
    "    name = hanzawa_name # 登録するPersonの名前を指定\n",
    ")\n",
    "\n",
    "owada = face_client.person_group_person.create(\n",
    "    person_group_id = hanzawa_group.person_group_id, # PersonGroupのIDを指定\n",
    "    name = owada_name # 登録するPersonの名前を指定\n",
    ")\n",
    "\n",
    "kurosaki = face_client.person_group_person.create(\n",
    "    person_group_id = hanzawa_group.person_group_id, # PersonGroupのIDを指定\n",
    "    name = kurosaki_name # 登録するPersonの名前を指定\n",
    ")\n",
    "\n",
    "print(hanzawa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hanzawa1': <azure.cognitiveservices.vision.face.models._models_py3.Person at 0x7ff9145245b0>,\n",
       " 'hanzawa2': <azure.cognitiveservices.vision.face.models._models_py3.Person at 0x7ff9145245b0>,\n",
       " 'hanzawa3': <azure.cognitiveservices.vision.face.models._models_py3.Person at 0x7ff9145245b0>,\n",
       " 'owada1': <azure.cognitiveservices.vision.face.models._models_py3.Person at 0x7ffb9ab21d60>,\n",
       " 'owada2': <azure.cognitiveservices.vision.face.models._models_py3.Person at 0x7ffb9ab21d60>,\n",
       " 'owada3': <azure.cognitiveservices.vision.face.models._models_py3.Person at 0x7ffb9ab21d60>,\n",
       " 'kurosaki1': <azure.cognitiveservices.vision.face.models._models_py3.Person at 0x7ffb9aacd190>,\n",
       " 'kurosaki2': <azure.cognitiveservices.vision.face.models._models_py3.Person at 0x7ffb9aacd190>,\n",
       " 'kurosaki3': <azure.cognitiveservices.vision.face.models._models_py3.Person at 0x7ffb9aacd190>}"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# person idをまとめておく\n",
    "person_id_dict = {'hanzawa1' : hanzawa, 'hanzawa2' : hanzawa, 'hanzawa3' : hanzawa, \n",
    "                  'owada1' : owada, 'owada2' : owada, 'owada3' : owada, \n",
    "                  'kurosaki1' : kurosaki, 'kurosaki2' : kurosaki, 'kurosaki3' : kurosaki}\n",
    "person_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hanzawa1 {'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': 'c13c5ca2-21ba-4ce1-bb73-7f1ec9460cf2', 'persisted_face_ids': None}\n",
      "hanzawa2 {'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': 'c13c5ca2-21ba-4ce1-bb73-7f1ec9460cf2', 'persisted_face_ids': None}\n",
      "hanzawa3 {'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': 'c13c5ca2-21ba-4ce1-bb73-7f1ec9460cf2', 'persisted_face_ids': None}\n",
      "owada1 {'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': 'ce1839b0-a222-4324-bcd0-dd000dd68940', 'persisted_face_ids': None}\n",
      "owada2 {'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': 'ce1839b0-a222-4324-bcd0-dd000dd68940', 'persisted_face_ids': None}\n",
      "owada3 {'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': 'ce1839b0-a222-4324-bcd0-dd000dd68940', 'persisted_face_ids': None}\n",
      "kurosaki1 {'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': '04d05382-c628-4402-9417-bf2c9d9bd8f1', 'persisted_face_ids': None}\n",
      "kurosaki2 {'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': '04d05382-c628-4402-9417-bf2c9d9bd8f1', 'persisted_face_ids': None}\n",
      "kurosaki3 {'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': '04d05382-c628-4402-9417-bf2c9d9bd8f1', 'persisted_face_ids': None}\n"
     ]
    }
   ],
   "source": [
    "for key, value in person_id_dict.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faceを登録"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [add_face_from_stream](https://docs.microsoft.com/ja-jp/python/api/azure-cognitiveservices-vision-face/azure.cognitiveservices.vision.face.operations.facelistoperations?view=azure-python#add-face-from-stream-face-list-id--image--user-data-none--target-face-none--detection-model--detection-01---custom-headers-none--raw-false--callback-none----operation-config-)  \n",
    "    人物グループに人物の顔を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(face_path_dict)\n",
    "keys = []\n",
    "face_list = []\n",
    "\n",
    "for i in face_path_dict.keys():\n",
    "    with open(face_path_dict[i], 'rb') as image:\n",
    "        keys.append(i)\n",
    "        face_list.append(face_client.person_group_person.add_face_from_stream(\n",
    "            person_group_id = hanzawa_group.person_group_id, \n",
    "            person_id = person_id_dict[i].person_id, \n",
    "            image = image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hanzawa1': <azure.cognitiveservices.vision.face.models._models_py3.PersistedFace at 0x7ff914144b80>,\n",
       " 'kurosaki1': <azure.cognitiveservices.vision.face.models._models_py3.PersistedFace at 0x7ff914722f40>,\n",
       " 'owada1': <azure.cognitiveservices.vision.face.models._models_py3.PersistedFace at 0x7ff914722be0>,\n",
       " 'hanzawa2': <azure.cognitiveservices.vision.face.models._models_py3.PersistedFace at 0x7ff9140a6d60>,\n",
       " 'kurosaki2': <azure.cognitiveservices.vision.face.models._models_py3.PersistedFace at 0x7ffb9af7e220>,\n",
       " 'owada3': <azure.cognitiveservices.vision.face.models._models_py3.PersistedFace at 0x7ffb9c5376d0>,\n",
       " 'owada2': <azure.cognitiveservices.vision.face.models._models_py3.PersistedFace at 0x7ff9142bdca0>,\n",
       " 'kurosaki3': <azure.cognitiveservices.vision.face.models._models_py3.PersistedFace at 0x7ff9141b89d0>,\n",
       " 'hanzawa3': <azure.cognitiveservices.vision.face.models._models_py3.PersistedFace at 0x7ff9142abe50>}"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# face_listとface_keyを紐付け\n",
    "face_images_dict = dict(zip(keys, face_list))\n",
    "face_images_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'persisted_face_id': '0e938b2a-b946-4c5f-8181-7804967b6e63', 'user_data': None}\n",
      "{'additional_properties': {}, 'persisted_face_id': '9125c48e-d04c-49b5-89df-5ba1962d1708', 'user_data': None}\n",
      "{'additional_properties': {}, 'persisted_face_id': 'd7c6e042-9786-4d0e-b979-67f84dfdb532', 'user_data': None}\n"
     ]
    }
   ],
   "source": [
    "print(face_images_dict['hanzawa1'])\n",
    "print(face_images_dict['hanzawa2'])\n",
    "print(face_images_dict['hanzawa3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最新のperson情報を取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [get(person_group_id, person_id, custom_headers=None, raw=False, **operation_config)](https://docs.microsoft.com/ja-jp/python/api/azure-cognitiveservices-vision-face/azure.cognitiveservices.vision.face.operations.persongrouppersonoperations?view=azure-python#get-person-group-id--person-id--custom-headers-none--raw-false----operation-config-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'name': 'hanzawa_naoki', 'user_data': None, 'person_id': 'c13c5ca2-21ba-4ce1-bb73-7f1ec9460cf2', 'persisted_face_ids': ['0e938b2a-b946-4c5f-8181-7804967b6e63', '9125c48e-d04c-49b5-89df-5ba1962d1708', 'd7c6e042-9786-4d0e-b979-67f84dfdb532']}\n"
     ]
    }
   ],
   "source": [
    "test = face_client.person_group_person.get(\n",
    "    person_group_id = hanzawa_group.person_group_id, \n",
    "    person_id = person_id_dict['hanzawa1'].person_id)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 顔写真の判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hanzawa1': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ffb21c01520>],\n",
       " 'kurosaki1': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ffb9c743bb0>],\n",
       " 'owada1': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ffb9aacf580>],\n",
       " 'hanzawa2': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ffb9ade19a0>],\n",
       " 'kurosaki2': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ff9141f6a90>],\n",
       " 'owada3': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ffb9ade1760>],\n",
       " 'owada2': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ff9144e2190>],\n",
       " 'kurosaki3': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ff9c3d2da00>],\n",
       " 'hanzawa3': [<azure.cognitiveservices.vision.face.models._models_py3.DetectedFace at 0x7ff9c3dc8ee0>]}"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_faces_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 登録済画像と検証用画像の比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [verify_face_to_person](https://docs.microsoft.com/ja-jp/python/api/azure-cognitiveservices-vision-face/azure.cognitiveservices.vision.face.operations.faceoperations?view=azure-python#verify-face-to-person-face-id--person-id--person-group-id-none--large-person-group-id-none--custom-headers-none--raw-false----operation-config-)  \n",
    "    FaceIDとPersonIDを比較する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しく比較する顔\n",
    "\n",
    "- [url1：半沢](https://www.crank-in.net/img/db/1406357_650.jpg)\n",
    "- [url2：大和田](https://pbs.twimg.com/profile_images/378800000485622414/7727f9842f6826183fd0a1e8e58200d1_400x400.jpeg)\n",
    "- [url3：黒崎](https://s.yimg.jp/images/gyao/trend/syst/2020/082638/01.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証用の画像の取得\n",
    "url1 = 'https://www.crank-in.net/img/db/1406357_650.jpg'\n",
    "url2 = 'https://pbs.twimg.com/profile_images/378800000485622414/7727f9842f6826183fd0a1e8e58200d1_400x400.jpeg'\n",
    "url3 = 'https://s.yimg.jp/images/gyao/trend/syst/2020/082638/01.jpg'\n",
    "\n",
    "# 検証用の顔検出\n",
    "hanzawa_test = face_client.face.detect_with_url(url1)\n",
    "owada_test = face_client.face.detect_with_url(url2)\n",
    "kurosaki_test = face_client.face.detect_with_url(url3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'is_identical': True, 'confidence': 0.70687}\n"
     ]
    }
   ],
   "source": [
    "# 検証用画像と半沢のperson idとの比較\n",
    "valified = face_client.face.verify_face_to_person(\n",
    "    face_id = hanzawa_test[0].face_id,\n",
    "    person_group_id = hanzawa_group.person_group_id,\n",
    "    person_id = person_id_dict['hanzawa1'].person_id\n",
    ")\n",
    "print(valified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'is_identical': True, 'confidence': 0.80649}\n"
     ]
    }
   ],
   "source": [
    "# 検証用画像と大和田のperson idとの比較\n",
    "valified = face_client.face.verify_face_to_person(\n",
    "    face_id = owada_test[0].face_id,\n",
    "    person_group_id = hanzawa_group.person_group_id,\n",
    "    person_id = person_id_dict['owada1'].person_id\n",
    ")\n",
    "print(valified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'is_identical': True, 'confidence': 0.75696}\n"
     ]
    }
   ],
   "source": [
    "# 検証用画像と大和田のperson idとの比較\n",
    "valified = face_client.face.verify_face_to_person(\n",
    "    face_id = kurosaki_test[0].face_id,\n",
    "    person_group_id = hanzawa_group.person_group_id,\n",
    "    person_id = person_id_dict['kurosaki1'].person_id\n",
    ")\n",
    "print(valified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valified.confidence > 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
